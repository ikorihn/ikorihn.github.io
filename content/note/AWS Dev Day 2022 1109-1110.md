---
title: AWS Dev Day 2022 1109-1110
date: 2023-01-04T16:36:00+09:00
tags:
- 2022/11/09
- meetup
---

## EKS on Fargate スタートアップの ‘次の3年’ を支えるためのインフラ技術

mBaaSからEKS on Fargateに移行した

* Go
* gRPC、Protocol Buffers
* サーバー管理したくないのでFargate
* Aurora Serverless
* AWS Cloud Development Kit (CDK)
* GitHub Actions
* ArgoCD

マイクロサービス化はしていないのにEKS使うメリット？

* k8sのエコシステムに乗っかる
* 構築できてしまえば運用は大変ではない

### AWS CDK

* 脱yaml TypeScriptやPythonなどコードで書けるようになる
* https://github.com/aws-quickstart/cdk-eks-blueprints を元にするとWell-ArchitectedなEKSを構築できる

### gRPC

* ALB関連オプションをgRPC向けにする
* ヘルスチェックへの応答をアプリケーションに作る

### Fargate用のDeployment

* 1 Pod = 1 Fargate Node
* Daemon Setは未対応
* Gravitonプロセッサ未対応
* シークレット管理のスマートな解決策が用意されていない
* Secrets Store CSI DriverはDaemon Setの使えないFargateでは使えない
* External Secrets Operatorを採用

## NewsPicksの「最高の開発者体験」への挑戦 〜Amazon ECSによる全面コンテナ化の軌跡〜

5,6人のSREチームで社内全体のSREなのうちと同じだ

もともとEC2(Auto Scaling Group)で運用していたが管理が大変
コンテナ化してECS管理したい

AWS CDKをTypeScriptで書いた
Constructという概念があって、インフラの構成を部品化して再利用できる
単体テストも書ける

EC2とECSを並行稼動
ALBで振り分け
リスナールールでターゲットグループの重みを調整して段々ECSに変えていく

CodeDeployでデプロイを行っていた
chat opsで、Lambdaを叩いてデプロイできるようになった

## AWS CDKでECS on FargateのCI/CDを実現する際の理想と現実

ECSとCDKのBlue/Greenデプロイが当時は不安定だったのでローリングアップデートを選択
カスタマイズも多い

ECSデプロイ高速化の手法はいくつかある
AWS SDKではhotswap deploymentがある
hotswapでは一時的にタスク数が0になるので検証でしか使えない

CDK + ECS on Fargateの良質なコンテンツはある

cdk-ecs-deploymentかカスタムリソース+CodeBuildが有力

## ２１年夏・２２年冬の国際的スポーツ大会の報道をサーバーレス化で乗り越えた！

時事通信社
オリンピックってワードは勝手に使っちゃいけないんだっけw

30,000PV/秒のリクエストに耐える負荷耐性
Lambdaのバースト同時実行数が上限を超えた場合のハンドリング

記者、カメラマン、パートナーからの情報を受信 -> CMSに格納 -> Webアプリケーションで表示、顧客サーバーに配信

### PHP Symfonyをサーバーレス化

Serverless Framework
スパイクアクセスが発生しやすいシステムでスケーリングから解放されるためサーバーレス化したかった
Auto Scalingでチューニングするのも難しい

### [LamdbaEdge](note/LamdbaEdge.md) を用いた [CloudFront](note/CloudFront.md) での応答性能改善

キャッシュ用のS3バケットを配置する
Origin Request時に実行する [LamdbaEdge](note/LamdbaEdge.md)
Userが [CloudFront](note/CloudFront.md) にリクエストすると [LamdbaEdge](note/LamdbaEdge.md) でS3バケットのキャッシュコンテンツかWebアプリのコンテンツを取得し返す

### 実行状態の可視化 [Lambda](note/AWS%20Lambda.md) から [Step Functions](note/Step%20Functions.md) への移行

一定条件のリトライ、状態による条件分岐、例外やエラー制御
1つのLambdaで実装したときに実行状態が見えづらい、タイムアウトに引っかかる可能性がある

### [AWS Lambda](note/AWS%20Lambda.md) のバースト同時実行数が上限に達した場合の対処

クウォータ引き上げでもバースト制限の初期値は上限緩和できない
同時実行数を引き上げていても、突発的に増えた場合はバーストの制限に引っかかる

* メモリサイズ別に実行時間を計測して、最適なメモリを割り当てる
* Lambdaのマルチリージョン化

## AWS LambdaにWebフレームワークを載せる

そもそもLambdaでWebフレームワークを動かすのはいいことなのか？

* Lambda関数の実装はシンプルに
  
  * API GatewayでANY Method / Proxy resourceを使ったモノリスになりがち
  * 最小権限の原則も破壊される
* cold start latencyの増加
  
  * 起動時間にかけるコストに見合わない
* 慣れ親しんだ開発体験
  
  * 社内でも開発経験者の確保が容易
* ポータビリティのため
  
  * ECSなどコンテナサービスに移植がしやすい

### LambdaにWAFを載せるためにされてきたこと

* 模倣体験の提供
  * Chaliceなど
* Runtime埋め込み系Adapter
  * Serverless Java
* Ahead-of-time Compilation

#### Runtime埋め込み系Adapter

Lambda Handlerはeventを引数に取る必要がある
eventオブジェクトとHTTPリクエストのgap
HandlerにAdapterを混入させてeventをhttp objectに変換してWAFにわたす

* 開発者がAdapterを意識する必要がある
* handler内にAdapterが存在するため、Artifactへのインストールが必要
* 完全なフレームワーク機能が使えない
* ポータビリティがない

#### Ahead-of-time Compilation(AOT)

GraalVM
AOT向けのフレームワークであるため使い慣れたフレームワークではない

### フレームワーク必要？

使い慣れた以外のメリットはあるの
リクエスト単位のisolationを持つLambdaでフレームワークを動かすとき、cold startのオーバーヘッドは無視できない
cold startにおけるリソース確保

Lambdaはリクエストごとに独立した実行環境 isolation ノイジーネイバー問題が発生しない

AWS Lambda Powertools がシンプル実装に対する非機能要件に対応している

それでも載せたいなら

* runtimeによらない
* レイテンシへの影響を最小限に
* Lambdaのイベント形式へのアダプターである

アダプターをRustで実装する
サイドカーパターンで実現

### AWS Lambdaでサイドカーパターンを利用する

サイドカーでロギング、リバースプロキシなど
Lambda Extensionを使用して、Lambda関数を拡張可能でこれをサイドカーパターンっぽく使える
Extensionをアダプターとして、eventをhttpオブジェクトに変換してhandlerにわたす

### AWS Lambda Web Adapter

LambdaでWebアプリを実行するためのツール
https://github.com/awslabs/aws-lambda-web-adapter

## DMMプラットフォームのマイクロサービス戦略 オーナーシップの落とし穴

* PHP、Java、JavaScript、Goなどなど
* テクノロジースタックがばらばら…あるある
* 他チームのアプリケーションログを見るために各AWSアカウントのログ閲覧権限が必要
* 各チームがオーナーシップをもっていて究極にサイロ化していた

サイロ化をなくすために…
全体最適化
テクノロジースタックの統一
開発ルールの標準化

そっかあ、技術領域はある程度決めてしまったほうがいいのか…うちはある程度そうなっているか。Apache + Tomcatな
全体最適化を満たそうとすると各チームの要件を満たせなくなってしまう
共通のテクノロジースタックを決めつつ、オーナーシップをどの程度持たせるか

### 全体最適化

オーナーシップの設計 = ガードレール

Amazonの例
社内に共通のビルドシステムやデプロイシステムが存在し、それを活用することで機械的にベストプラクティスを浸透させる

全体最適化を進めるには
専門チームがフルコミットしないと難しい
全員の承認を得ていくのは難しいのである程度トップダウンで進める

未来に向けた最適化なので、短期的には効率は下がる

### SREチームの最適化

運用モデル
アプリケーションチームがインフラも面倒見る体制から、
SREチームが各チームに向けて必要な仕組みを提供して、共通インフラを用意する

ログ/メトリクスの共通ルールを定義
Datadogを導入した

オンプレからクラウドへの移行プロジェクトが動いていた
そこにk8sを導入した
マルチクラウドなのでAWSではEKS、他のパブリッククラウドでもk8sを構築
CDツールはArgoCD、コンテナレジストリも共通

### 各チームの独立性が失われる

共通インフラに統一すると、各チームの独立性が失われる

共通インフラの利用者は単一のGitリポジトリでTerraform管理
k8sのマニフェストについても単一リポジトリで管理
GitHubにはコードオーナーというのがあって、ほかチームのマニフェストファイルを変更することはない
マニフェストの管理はアプリケーション開発者のほうで行っている
SREチームではドキュメントの整備
仕組みを提供して自由に使ってもらう

### オーナーシップ vs 共通化

性善説ベースで権限管理している
技術の詳細をどこまで隠蔽するか？

* k8sのマニフェストファイルの隠蔽 kustomize等で開発者は少しだけ変更すれば使えるようにするのか、詳細を隠蔽すると細かい要件を満たしにくくなる
* DMMでは隠蔽しないで開発者に素の状態で使ってもらってる
* 自分たちが使う技術は抽象化せずきちんと身につけてもらうという方針

開発チームが解決してほしい課題もSREチームに頼ってしまうケースがある
開発者が自立して対処できるようになってもらおうとしている

### 状況

バックエンドはGo、フロントエンドはTypeScript
CI、DB選定、クラウド選定は開発者
